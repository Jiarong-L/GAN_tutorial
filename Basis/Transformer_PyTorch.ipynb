{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e17e5ba-7df8-4816-81b4-807929c3283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a49f40-4a32-42e6-9d31-558fd11dfbde",
   "metadata": {},
   "source": [
    "Torch Example: https://github.com/Jiarong-L/BioDL/blob/main/1.%20Protein_seq_Classifier/VFDB_Transformer.ipynb\n",
    "\n",
    "1. PositionalEncoding\n",
    "2. nn.TransformerEncoderLayer + nn.\n",
    "\n",
    "[图示详解](https://zhuanlan.zhihu.com/p/338817680)：Attention矩阵的每一行，对应一个单词的表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70601b-a588-4ea5-99e9-a0547a7bb136",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "$attention = softmax(\\frac{QK'}{\\sqrt{K_{embDim}}})V$\n",
    "\n",
    "$Multi Head Attention = weightedSum(Attention1,Attention2,Attention3,...)$\n",
    "\n",
    "\n",
    "**Transformer** Figure1:  https://arxiv.org/abs/1706.03762 \n",
    "\n",
    "**Positional Encoding**: pos in sequence, i in input_Matrix   \n",
    "PE(pos,2i) = $sin(pos/10000^{2i/dModel})$   \n",
    "PE(pos,2i+1) = $cos(pos/10000^{2i/dModel})$  \n",
    "PE(others) = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27105c5d-daa5-4785-a033-ab0dbd26e75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.388998</td>\n",
       "      <td>1.460099</td>\n",
       "      <td>1.954951</td>\n",
       "      <td>2.715260</td>\n",
       "      <td>1.949436</td>\n",
       "      <td>2.594846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.336696</td>\n",
       "      <td>1.698741</td>\n",
       "      <td>1.801907</td>\n",
       "      <td>2.615653</td>\n",
       "      <td>2.100490</td>\n",
       "      <td>2.675136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.501407</td>\n",
       "      <td>1.562851</td>\n",
       "      <td>2.035426</td>\n",
       "      <td>2.758198</td>\n",
       "      <td>2.033766</td>\n",
       "      <td>2.696152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.591722</td>\n",
       "      <td>1.581702</td>\n",
       "      <td>2.120205</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>2.056615</td>\n",
       "      <td>2.741807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  2.388998  1.460099  1.954951  2.715260  1.949436  2.594846\n",
       "1  2.336696  1.698741  1.801907  2.615653  2.100490  2.675136\n",
       "2  2.501407  1.562851  2.035426  2.758198  2.033766  2.696152\n",
       "3  2.591722  1.581702  2.120205  2.803730  2.056615  2.741807"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 6\n",
    "word_emb = 5\n",
    "word_num = 4\n",
    "\n",
    "WQ = np.random.rand(word_emb,d_model)\n",
    "WK = np.random.rand(word_emb,d_model)    ## dk=d_model\n",
    "WV = np.random.rand(word_emb,d_model)\n",
    "\n",
    "\n",
    "input = pd.DataFrame({\n",
    "    \"<sos>\": np.random.randn(word_emb),\n",
    "    \"<you>\": np.random.randn(word_emb),\n",
    "    \"<like>\": np.random.randn(word_emb),\n",
    "    \"<swimming>\": np.random.randn(word_emb),\n",
    "}).T\n",
    "\n",
    "\n",
    "Q = input @ WQ    ## (query=word_num, word_emb)\n",
    "K = input @ WK    ##   (key=word_num, word_emb)\n",
    "V = input @ WV    ## (value=word_num, word_emb)\n",
    "\n",
    "def softmax(M):\n",
    "    M = M.values\n",
    "    M = M - np.max(M, axis=1, keepdims=True)\n",
    "    return np.exp(M)/np.sum(np.exp(M),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "Att = softmax(Q @ K.T /np.sqrt(d_model)) @ V\n",
    "## Multi-head = np.concatenate(Att_1,Att_2,Att_3)WH    ##-->WH is each attention's weight\n",
    "\n",
    "Att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2037c7-9d28-420a-b9a2-b4f6e6da456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3890, 1.4601, 1.9550, 2.7153, 1.9494, 2.5948],\n",
       "        [2.3367, 1.6987, 1.8019, 2.6157, 2.1005, 2.6751],\n",
       "        [2.5014, 1.5629, 2.0354, 2.7582, 2.0338, 2.6962],\n",
       "        [2.5917, 1.5817, 2.1202, 2.8037, 2.0566, 2.7418]], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pytorch Function\n",
    "## https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "\n",
    "torch.nn.functional.scaled_dot_product_attention(torch.tensor(Q.values), torch.tensor(K.values), torch.tensor(V.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae0cfa-9542-4d15-97b5-f6524e99aa1d",
   "metadata": {},
   "source": [
    "### Mask\n",
    "\n",
    "```\n",
    "\n",
    "Encoder:    (pos_emb,seq_source) ------------ [attention] ---------- [Forward] ---- (feature)\n",
    "\n",
    "Decoder:    (feature,pos_emb,seq_target) ---- [masked_attention] --- [Forward] ---- (next word)\n",
    "\n",
    "\n",
    "(feature,pos_emb,'<sos>') ---> Decoder ---> 'you'\n",
    "(feature,pos_emb,'<sos> you') ---> Decoder ---> 'like'\n",
    "(feature,pos_emb,'<sos> you like') ---> Decoder ---> 'swimming'\n",
    "(feature,pos_emb,'<sos> you like swimming') ---> Decoder ---> '<eos>'\n",
    "```\n",
    "\n",
    "Decoder训练时已知晓生成的序列，可以一次计算它的attention矩阵；但输入时需要mask掉暂未使用的部分：Masked_QK * V 得到 Masked Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a868d53a-6efa-4349-b5c7-a1c92e04ce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;sos&gt;</th>\n",
       "      <th>&lt;you&gt;</th>\n",
       "      <th>&lt;like&gt;</th>\n",
       "      <th>&lt;swimming&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;sos&gt;</th>\n",
       "      <td>8.608641</td>\n",
       "      <td>3.832589</td>\n",
       "      <td>7.741274</td>\n",
       "      <td>7.718041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;you&gt;</th>\n",
       "      <td>9.188842</td>\n",
       "      <td>3.664324</td>\n",
       "      <td>8.477303</td>\n",
       "      <td>9.482574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;like&gt;</th>\n",
       "      <td>12.407758</td>\n",
       "      <td>4.827441</td>\n",
       "      <td>10.881397</td>\n",
       "      <td>11.590250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;swimming&gt;</th>\n",
       "      <td>14.187342</td>\n",
       "      <td>4.906694</td>\n",
       "      <td>11.986032</td>\n",
       "      <td>13.136671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                <sos>     <you>     <like>  <swimming>\n",
       "<sos>        8.608641  3.832589   7.741274    7.718041\n",
       "<you>        9.188842  3.664324   8.477303    9.482574\n",
       "<like>      12.407758  4.827441  10.881397   11.590250\n",
       "<swimming>  14.187342  4.906694  11.986032   13.136671"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt = Q @ K.T / np.sqrt(d_model)\n",
    "QKt_tensor = torch.tensor(QKt.values)\n",
    "QKt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a434f38c-fd15-47a2-99c4-d29f46ddcb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones_like(QKt_tensor))\n",
    "mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e400147-ba84-4d07-bd66-51765cd3194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6086,    -inf,    -inf,    -inf],\n",
       "        [ 9.1888,  3.6643,    -inf,    -inf],\n",
       "        [12.4078,  4.8274, 10.8814,    -inf],\n",
       "        [14.1873,  4.9067, 11.9860, 13.1367]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt_att = QKt_tensor.masked_fill_(mask == 0,-torch.inf)\n",
    "QKt_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ef8311-0650-4e16-85d8-8ea4faa7568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [9.9603e-01, 3.9719e-03, 0.0000e+00, 0.0000e+00],\n",
       "        [8.2113e-01, 4.1910e-04, 1.7845e-01, 0.0000e+00],\n",
       "        [6.8472e-01, 6.3823e-05, 7.5770e-02, 2.3945e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(QKt_att, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af24d2-8055-4033-b407-2bbf9d975506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798a20b-0ed9-41b5-9b23-081581370770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ade946-dce3-4c1e-b2da-45deb6a18797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
