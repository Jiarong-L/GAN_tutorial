{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d17e74-ab81-413c-b123-7290812112ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cfc77-df34-49b5-992d-035aadedc44b",
   "metadata": {},
   "source": [
    "## 文本处理\n",
    "\n",
    "1. 分词/Tokenization\n",
    "```py\n",
    "list(str)     ## 1. by letter\n",
    "str.split()   ## 2. by word\n",
    "...........   ## 3. NLTK-ngrams: 有点类似k-mer，将n个相邻词合并为一个词段，稍后embed这个词段\n",
    "```\n",
    "2. 创建词表 vocab\n",
    "\n",
    "3. 词表向量化：tf-idf/one-hot/散列编码/embedding\n",
    "\n",
    "\n",
    "[gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) 进行word2vec: 根据语料中词汇一起出现的频率（co-occurrence），进行word embedding。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bccc6",
   "metadata": {},
   "source": [
    "### recall: word2vec \n",
    "\n",
    "根据语料中词汇一起出现的频率（co-occurrence），进行word embedding。\n",
    "\n",
    "工具包：[gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)\n",
    "\n",
    "![](word2vec/1.PNG) \n",
    "\n",
    "参考\n",
    "\n",
    "[1-原文] https://arxiv.org/pdf/1301.3781.pdf  \n",
    "[2-优化Skip-gram] https://arxiv.org/pdf/1310.4546.pdf  \n",
    "[3-释意] https://arxiv.org/pdf/1411.2738.pdf  \n",
    "[4-帮助理解-数学描述] https://zhuanlan.zhihu.com/p/595381757  \n",
    "[5-帮助理解-负采样] http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172d0bd5-5eaf-4ef1-a071-75e2427102f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"Sing in me, Muse, and through me tell the story of that man skilled in all ways of contending, the wanderer, harried for years on end, after he plundered the stronghold on the proud height of Troy\"\n",
    "\n",
    "for c in string.punctuation:\n",
    "    str = str.replace(c,' ').replace('  ',' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3a0218-03e8-483b-9c88-0e4d793ec3f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 21, 0, 1, 13, 17, 0, 11, 20, 6, 10, 15, 8, 2, 21, 16, 27, 10, 14, 20, 23, 25, 26, 19, 7, 22, 3, 12, 4, 20, 18, 7, 20, 28, 5, 10, 24]\n"
     ]
    }
   ],
   "source": [
    "vocab = dict([(v,k) for k,v in enumerate(set(str.split()))])\n",
    "s = [vocab.get(v) for v in str.split()]  \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721fc396-3533-4c38-9522-8298eb1c5ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. one-hot Mtx, each line for a word\n",
    "b = np.zeros((len(s),len(vocab))) \n",
    "for index,v in enumerate(s):\n",
    "    b[index,v] = 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b0a4f5-17b3-4308-b4e0-bb2104499fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Embedding, each line for a word\n",
    "emb_layer = torch.nn.Embedding(len(vocab),20)\n",
    "emb = emb_layer(torch.LongTensor(s))\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f687e-6959-44e4-a473-8a622957a25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afdec691-88ee-483c-8e73-aa714d645c88",
   "metadata": {},
   "source": [
    "## TorchText\n",
    "\n",
    "```\n",
    "torchtext.datasets.* 提供常见数据集  但是与torch==2.3.0不相容；可以直接从官网链接直接下载tar\n",
    "\n",
    "且下载容易出错，建议转换为list()后check一下大小、标签set\n",
    "\n",
    "https://pytorch.org/text/stable/datasets.html\n",
    "```\n",
    "* 各种文本处理工具: https://pytorch.org/text/stable/data_utils.html   \n",
    "* 忘记 vocab.set_default_index 后续调用如果遇到不在vocab中的词后程序会崩溃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5799132e-d9b7-4f88-ae61-20462a224e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.0+cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext     ## pip install torchtext; pip install torchdata\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db3ff0c2-03e8-4ac5-ae00-a090e7b4a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(torchtext.datasets.IMDB(split='train',root=r'L:\\Datasets'))\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for (_,text) in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(test_data),specials=['<pad>','<unk>'],min_freq=3)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae86a18-176c-4f1b-b83a-fa378c137a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([k[0] for k in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48296c66-6337-418a-aa13-c9ce3c735903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9cecbc-a8bc-4845-aefe-ad9a05ede43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262b95c3-28e9-4944-9a27-c345937d822d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'heard',\n",
       " 'that',\n",
       " 'at',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'seized',\n",
       " 'by',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'customs',\n",
       " 'if',\n",
       " 'it',\n",
       " 'ever',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'this',\n",
       " 'country',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'being',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'films',\n",
       " 'considered',\n",
       " 'controversial',\n",
       " 'i',\n",
       " 'really',\n",
       " 'had',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'for',\n",
       " 'myself',\n",
       " '.',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'is',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'young',\n",
       " 'swedish',\n",
       " 'drama',\n",
       " 'student',\n",
       " 'named',\n",
       " 'lena',\n",
       " 'who',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'she',\n",
       " 'can',\n",
       " 'about',\n",
       " 'life',\n",
       " '.',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'she',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'focus',\n",
       " 'her',\n",
       " 'attentions',\n",
       " 'to',\n",
       " 'making',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'documentary',\n",
       " 'on',\n",
       " 'what',\n",
       " 'the',\n",
       " 'average',\n",
       " 'swede',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'certain',\n",
       " 'political',\n",
       " 'issues',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'vietnam',\n",
       " 'war',\n",
       " 'and',\n",
       " 'race',\n",
       " 'issues',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'in',\n",
       " 'between',\n",
       " 'asking',\n",
       " 'politicians',\n",
       " 'and',\n",
       " 'ordinary',\n",
       " 'denizens',\n",
       " 'of',\n",
       " 'stockholm',\n",
       " 'about',\n",
       " 'their',\n",
       " 'opinions',\n",
       " 'on',\n",
       " 'politics',\n",
       " ',',\n",
       " 'she',\n",
       " 'has',\n",
       " 'sex',\n",
       " 'with',\n",
       " 'her',\n",
       " 'drama',\n",
       " 'teacher',\n",
       " ',',\n",
       " 'classmates',\n",
       " ',',\n",
       " 'and',\n",
       " 'married',\n",
       " 'men',\n",
       " '.',\n",
       " 'what',\n",
       " 'kills',\n",
       " 'me',\n",
       " 'about',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'is',\n",
       " 'that',\n",
       " '40',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'this',\n",
       " 'was',\n",
       " 'considered',\n",
       " 'pornographic',\n",
       " '.',\n",
       " 'really',\n",
       " ',',\n",
       " 'the',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nudity',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'few',\n",
       " 'and',\n",
       " 'far',\n",
       " 'between',\n",
       " ',',\n",
       " 'even',\n",
       " 'then',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'shot',\n",
       " 'like',\n",
       " 'some',\n",
       " 'cheaply',\n",
       " 'made',\n",
       " 'porno',\n",
       " '.',\n",
       " 'while',\n",
       " 'my',\n",
       " 'countrymen',\n",
       " 'mind',\n",
       " 'find',\n",
       " 'it',\n",
       " 'shocking',\n",
       " ',',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nudity',\n",
       " 'are',\n",
       " 'a',\n",
       " 'major',\n",
       " 'staple',\n",
       " 'in',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'even',\n",
       " 'ingmar',\n",
       " 'bergman',\n",
       " ',',\n",
       " 'arguably',\n",
       " 'their',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'good',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'john',\n",
       " 'ford',\n",
       " ',',\n",
       " 'had',\n",
       " 'sex',\n",
       " 'scenes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'films',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " 'commend',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'any',\n",
       " 'sex',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " 'is',\n",
       " 'shown',\n",
       " 'for',\n",
       " 'artistic',\n",
       " 'purposes',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'just',\n",
       " 'to',\n",
       " 'shock',\n",
       " 'people',\n",
       " 'and',\n",
       " 'make',\n",
       " 'money',\n",
       " 'to',\n",
       " 'be',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'pornographic',\n",
       " 'theaters',\n",
       " 'in',\n",
       " 'america',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'film',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'wanting',\n",
       " 'to',\n",
       " 'study',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'potatoes',\n",
       " '(',\n",
       " 'no',\n",
       " 'pun',\n",
       " 'intended',\n",
       " ')',\n",
       " 'of',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'but',\n",
       " 'really',\n",
       " ',',\n",
       " 'this',\n",
       " 'film',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'plot',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(yield_tokens(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25fc95b-55e8-4630-981e-7437b66b38b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['am']   ## index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67db4fe6-c07b-4134-a703-01cf323598a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 125]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['i','love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53fec8f0-f06e-4e5c-87ca-d7280920b0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fdd0c44-8058-4cee-a31f-1cd5fc4cabff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 1568,\n",
       " 13,\n",
       " 246,\n",
       " 35468,\n",
       " 43,\n",
       " 64,\n",
       " 398,\n",
       " 1135,\n",
       " 92,\n",
       " 7,\n",
       " 37,\n",
       " 2,\n",
       " 7126,\n",
       " 15,\n",
       " 3363,\n",
       " 11,\n",
       " 60,\n",
       " 11,\n",
       " 17,\n",
       " 94,\n",
       " 629,\n",
       " 12,\n",
       " 6921,\n",
       " 3,\n",
       " 13,\n",
       " 87,\n",
       " 553,\n",
       " 15,\n",
       " 38,\n",
       " 94,\n",
       " 11,\n",
       " 17,\n",
       " 20193,\n",
       " 40,\n",
       " 1225,\n",
       " 3,\n",
       " 16,\n",
       " 3,\n",
       " 9263,\n",
       " 51,\n",
       " 11,\n",
       " 131,\n",
       " 780,\n",
       " 8,\n",
       " 2480,\n",
       " 14,\n",
       " 682,\n",
       " 4,\n",
       " 1575,\n",
       " 118,\n",
       " 6,\n",
       " 342,\n",
       " 7,\n",
       " 114,\n",
       " 1160,\n",
       " 3052,\n",
       " 13,\n",
       " 72,\n",
       " 75,\n",
       " 8,\n",
       " 74,\n",
       " 14,\n",
       " 19,\n",
       " 537,\n",
       " 3,\n",
       " 2,\n",
       " 121,\n",
       " 10,\n",
       " 5959,\n",
       " 194,\n",
       " 6,\n",
       " 191,\n",
       " 3862,\n",
       " 474,\n",
       " 1424,\n",
       " 766,\n",
       " 4314,\n",
       " 42,\n",
       " 489,\n",
       " 8,\n",
       " 834,\n",
       " 287,\n",
       " 61,\n",
       " 58,\n",
       " 50,\n",
       " 127,\n",
       " 3,\n",
       " 12,\n",
       " 826,\n",
       " 61,\n",
       " 489,\n",
       " 8,\n",
       " 1132,\n",
       " 47,\n",
       " 11859,\n",
       " 8,\n",
       " 257,\n",
       " 56,\n",
       " 441,\n",
       " 7,\n",
       " 669,\n",
       " 28,\n",
       " 54,\n",
       " 2,\n",
       " 863,\n",
       " 29737,\n",
       " 209,\n",
       " 50,\n",
       " 781,\n",
       " 1001,\n",
       " 1304,\n",
       " 147,\n",
       " 18,\n",
       " 2,\n",
       " 2675,\n",
       " 337,\n",
       " 5,\n",
       " 1510,\n",
       " 1304,\n",
       " 12,\n",
       " 2,\n",
       " 2359,\n",
       " 1592,\n",
       " 3,\n",
       " 12,\n",
       " 203,\n",
       " 2182,\n",
       " 7271,\n",
       " 5,\n",
       " 1919,\n",
       " 19586,\n",
       " 7,\n",
       " 21478,\n",
       " 50,\n",
       " 73,\n",
       " 4656,\n",
       " 28,\n",
       " 2381,\n",
       " 4,\n",
       " 61,\n",
       " 52,\n",
       " 402,\n",
       " 20,\n",
       " 47,\n",
       " 474,\n",
       " 1692,\n",
       " 4,\n",
       " 8135,\n",
       " 4,\n",
       " 5,\n",
       " 999,\n",
       " 347,\n",
       " 3,\n",
       " 54,\n",
       " 1080,\n",
       " 78,\n",
       " 50,\n",
       " 13,\n",
       " 246,\n",
       " 35468,\n",
       " 10,\n",
       " 15,\n",
       " 1614,\n",
       " 161,\n",
       " 587,\n",
       " 4,\n",
       " 14,\n",
       " 17,\n",
       " 1160,\n",
       " 8206,\n",
       " 3,\n",
       " 72,\n",
       " 4,\n",
       " 2,\n",
       " 402,\n",
       " 5,\n",
       " 1000,\n",
       " 145,\n",
       " 31,\n",
       " 175,\n",
       " 5,\n",
       " 242,\n",
       " 203,\n",
       " 4,\n",
       " 63,\n",
       " 101,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 29,\n",
       " 330,\n",
       " 45,\n",
       " 56,\n",
       " 6655,\n",
       " 100,\n",
       " 4461,\n",
       " 3,\n",
       " 143,\n",
       " 64,\n",
       " 23465,\n",
       " 348,\n",
       " 172,\n",
       " 11,\n",
       " 1574,\n",
       " 4,\n",
       " 12,\n",
       " 635,\n",
       " 402,\n",
       " 5,\n",
       " 1000,\n",
       " 31,\n",
       " 6,\n",
       " 663,\n",
       " 10198,\n",
       " 12,\n",
       " 3862,\n",
       " 437,\n",
       " 3,\n",
       " 63,\n",
       " 14516,\n",
       " 4441,\n",
       " 4,\n",
       " 4634,\n",
       " 73,\n",
       " 1476,\n",
       " 8,\n",
       " 57,\n",
       " 176,\n",
       " 435,\n",
       " 304,\n",
       " 1721,\n",
       " 4,\n",
       " 75,\n",
       " 402,\n",
       " 145,\n",
       " 12,\n",
       " 32,\n",
       " 114,\n",
       " 3,\n",
       " 13,\n",
       " 91,\n",
       " 12612,\n",
       " 2,\n",
       " 1023,\n",
       " 19,\n",
       " 2,\n",
       " 198,\n",
       " 15,\n",
       " 107,\n",
       " 402,\n",
       " 608,\n",
       " 12,\n",
       " 2,\n",
       " 23,\n",
       " 10,\n",
       " 608,\n",
       " 19,\n",
       " 1577,\n",
       " 5042,\n",
       " 251,\n",
       " 79,\n",
       " 49,\n",
       " 8,\n",
       " 1498,\n",
       " 85,\n",
       " 5,\n",
       " 105,\n",
       " 280,\n",
       " 8,\n",
       " 34,\n",
       " 608,\n",
       " 12,\n",
       " 8206,\n",
       " 2181,\n",
       " 12,\n",
       " 835,\n",
       " 3,\n",
       " 13,\n",
       " 246,\n",
       " 35468,\n",
       " 10,\n",
       " 6,\n",
       " 57,\n",
       " 23,\n",
       " 19,\n",
       " 256,\n",
       " 1752,\n",
       " 8,\n",
       " 2058,\n",
       " 2,\n",
       " 3751,\n",
       " 5,\n",
       " 18993,\n",
       " 25,\n",
       " 65,\n",
       " 5230,\n",
       " 1400,\n",
       " 24,\n",
       " 7,\n",
       " 3862,\n",
       " 437,\n",
       " 3,\n",
       " 22,\n",
       " 72,\n",
       " 4,\n",
       " 14,\n",
       " 23,\n",
       " 159,\n",
       " 9,\n",
       " 27,\n",
       " 33,\n",
       " 81,\n",
       " 7,\n",
       " 6,\n",
       " 121,\n",
       " 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(next(yield_tokens(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1a4ed-6fe5-4998-802d-1224c20dfc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64633182-4d0c-484b-bab5-30149cd543fe",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "设定 collate_fn batch ...\n",
    "\n",
    "此例是为了torch.nn.EmbeddingBag的输入做准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd52589-9700-42b3-a816-0fdf9cf25db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(data_batch):\n",
    "    label_lst, text_lst, offset_lst = [],[],[]\n",
    "    for _label, _text in data_batch:\n",
    "        label_lst.append(_label-1)  ## {1,2} => {0,1}\n",
    "        tk_text = vocab(tokenizer(_text))\n",
    "        text_lst.append(torch.tensor(tk_text,dtype=torch.int64))\n",
    "        offset_lst.append(len(tk_text))\n",
    "    label_lst = torch.tensor(label_lst)\n",
    "    text_lst = torch.cat(text_lst)\n",
    "    offsets = torch.cat((torch.tensor([0]), torch.tensor(offset_lst[:-1]).cumsum(dim=0) ))\n",
    "    return label_lst.to(device),text_lst.to(device),offsets.to(device)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "489fbf14-df06-4356-9584-675e24da9a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0'),\n",
       " tensor([  37,    7,  592,  ...,   29, 1145,    3], device='cuda:0'),\n",
       " tensor([    0,   273,   442,   889,  1060,  1610,  1788,  2090,  2261,  2474,\n",
       "          2834,  3005,  3210,  3507,  3601,  3871,  4230,  4279,  4418,  4685,\n",
       "          5132,  5275,  5748,  6026,  6249,  6829,  7435,  7567,  7640,  7827,\n",
       "          8068,  8224,  8333,  8493,  8671,  8965,  9134,  9257,  9473,  9864,\n",
       "         10135, 10382, 10620, 10788, 10957, 11020, 11395, 11559, 11873, 11977,\n",
       "         12377, 13530, 13651, 13800, 13973, 14113, 14568, 14668, 14914, 15100,\n",
       "         15330, 15622, 15796, 16033], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tt in test_dl:\n",
    "    break\n",
    "\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3eccba-8344-4fab-a67b-08b4385e87da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4afa135-1fb0-44d7-b7c2-371dd06e774a",
   "metadata": {},
   "source": [
    "## EmbeddingBag Model\n",
    "\n",
    "```\n",
    "## torch.nn.EmbeddingBag 接受词表进行 Embedding，并对 Embedding 输出进行聚合：求和/均值/最大值/。。。\n",
    "## 可以将一个批次中全部文本合并为一个长序列，并记录其中每一条文本的 **偏移值**（其所在位置）\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568a6427-af9d-4ab4-ab77-20c9a8fe03ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifyModel(\n",
       "  (Embed_Bag): EmbeddingBag(40252, 100, mode='mean')\n",
       "  (fc): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextClassifyModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.Embed_Bag = torch.nn.EmbeddingBag(vocab_size,embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, 2)\n",
    "    def forward(self, text, offset):\n",
    "        embd = self.Embed_Bag(text,offset)\n",
    "        return self.fc(embd)\n",
    "\n",
    "model = TextClassifyModel(len(vocab),100).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2090d471-36c5-43a8-949d-ef168f2f324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de58ab8-05f4-470e-8ccf-d0c37b1ee88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    lossSum = 0\n",
    "    model.train()                                    ### set training mode\n",
    "    for label_lst,text_lst,offsets in dataloader:\n",
    "        # Compute prediction error\n",
    "        pred = model(text_lst,offsets)\n",
    "        loss = loss_fn(pred, label_lst)\n",
    "        lossSum += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avgTrainingLoss = lossSum/len(dataloader)\n",
    "    return avgTrainingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59ccb3a-e3ae-43ba-82d1-6b304f35ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1----Train Loss:: 0.671038\n",
      "Epoch 2----Train Loss:: 0.646020\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    avgTrainingLoss = train(test_dl, model, loss_fn, optimizer)\n",
    "    print(f'Epoch {t+1}----Train Loss:: {avgTrainingLoss:>7f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d99bed-55c7-4610-ba54-7dea868237d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f959a5a-42d4-4207-82f1-5af14511b186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
